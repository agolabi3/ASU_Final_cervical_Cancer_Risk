# -*- coding: utf-8 -*-
"""ASU_Cervical_Cancer_Risk_Final

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15XImERqPPKnE3YD8CY2bm7PIARw-QhoI
"""

# ==========================================================
# CIS 508 Final Project - Cervical Cancer Risk Prediction
# Colab + Databricks MLflow
# - Uploads cervical cancer dataset
# - Cleans data
# - Runs ALL required classification models:
#   * Decision tree, Logistic regression, SVM, Neural net,
#     Naive Bayes, Random Forest, XGBoost, KNN, Ensemble
# - Hyperparameter tuning: 2‚Äì3 hyperparams per model
# - Logs metrics & params to Databricks MLflow
# ==========================================================

# 0. Install dependencies (Colab only)
!pip install -q mlflow databricks-sdk xgboost scikit-learn pandas numpy

# 1. Imports
import os
import itertools
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier

from xgboost import XGBClassifier

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    classification_report,
)

import mlflow
import mlflow.sklearn

from google.colab import files, userdata

# 2. Upload dataset
print("üìÇ Please upload 'risk_factors_cervical_cancer.csv' from Kaggle.")
uploaded = files.upload()
csv_name = list(uploaded.keys())[0]
df = pd.read_csv(csv_name)
print(f"\n‚úÖ Loaded {csv_name} with shape {df.shape}")

# 3. Data cleaning & preparation
df.replace("?", np.nan, inplace=True)
df = df.apply(pd.to_numeric, errors="ignore")
df.dropna(subset=["Biopsy"], inplace=True)

X = df.drop(columns=["Biopsy"])
y = df["Biopsy"]

imputer = SimpleImputer(strategy="median")
X_imputed = imputer.fit_transform(X)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

print("‚úÖ Data cleaned, imputed, scaled, and split.")
print("   Train shape:", X_train.shape, " Test shape:", X_test.shape)

# 4. Configure Databricks MLflow
try:
    os.environ["DATABRICKS_HOST"] = userdata.get("DATABRICKS_HOST")
    os.environ["DATABRICKS_TOKEN"] = userdata.get("DATABRICKS_TOKEN")
except Exception as e:
    print("‚ö†Ô∏è Could not load Colab userdata secrets. Make sure they are set.")
    print(e)

mlflow.set_tracking_uri("databricks")
experiment_name = "/Users/agolabi3@asu.edu/CervicalCancer_FinalProject"
mlflow.set_experiment(experiment_name)

print("Tracking URI:", mlflow.get_tracking_uri())
print("Experiment: ", experiment_name)

# 5. Helper: create param grid (Cartesian product)
def make_param_grid(param_dict):
    keys = list(param_dict.keys())
    values = list(param_dict.values())
    combos = list(itertools.product(*values))
    grid = []
    for combo in combos:
        grid.append({k: v for k, v in zip(keys, combo)})
    return grid

# Helper: evaluate model
def evaluate_model(name, y_true, y_pred):
    acc  = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, zero_division=0)
    rec  = recall_score(y_true, y_pred, zero_division=0)
    f1   = f1_score(y_true, y_pred, zero_division=0)

    print(f"\nüìä {name} Results")
    print("----------------------------")
    print(f"Accuracy : {acc:.3f}")
    print(f"Precision: {prec:.3f}")
    print(f"Recall   : {rec:.3f}")
    print(f"F1-score : {f1:.3f}")
    print("\nClassification report:")
    print(classification_report(y_true, y_pred, zero_division=0))

    return {
        "accuracy": acc,
        "precision": prec,
        "recall": rec,
        "f1_score": f1,
    }

# Track best model by recall
best_overall = {
    "family": None,
    "run_name": None,
    "params": None,
    "metrics": {"recall": -1.0},
}

def update_best(family, run_name, params, metrics):
    global best_overall
    if metrics["recall"] > best_overall["metrics"]["recall"]:
        best_overall = {
            "family": family,
            "run_name": run_name,
            "params": params,
            "metrics": metrics,
        }

# 6. Define model families and hyperparameter grids

models_and_grids = []

# 1) Classification Tree
dt_param_dict = {
    "max_depth": [3, 5],
    "min_samples_split": [2, 5],
    "criterion": ["gini", "entropy"],
}
models_and_grids.append(("DecisionTree", DecisionTreeClassifier, dt_param_dict))

# 2) Logistic Regression
log_param_dict = {
    "C": [0.1, 1.0, 10.0],
    "class_weight": [None, "balanced"],
    "penalty": ["l2"],  # keep consistent with lbfgs
}
models_and_grids.append(("LogisticRegression", LogisticRegression, log_param_dict))

# 3) SVM
svm_param_dict = {
    "C": [0.1, 1.0],
    "kernel": ["linear", "rbf"],
    "gamma": ["scale", "auto"],
}
models_and_grids.append(("SVM", SVC, svm_param_dict))

# 4) Neural Network (MLP)
ann_param_dict = {
    "hidden_layer_sizes": [(16,), (32, 16)],
    "alpha": [0.0001, 0.001],
    "activation": ["relu", "tanh"],
}
models_and_grids.append(("ANN", MLPClassifier, ann_param_dict))

# 5) Naive Bayes
nb_param_dict = {
    "var_smoothing": [1e-9, 1e-7],
}
models_and_grids.append(("NaiveBayes", GaussianNB, nb_param_dict))

# 6) Random Forest
rf_param_dict = {
    "n_estimators": [100, 200],
    "max_depth": [None, 5],
    "max_features": ["sqrt", "log2"],
}
models_and_grids.append(("RandomForest", RandomForestClassifier, rf_param_dict))

# 7) XGBoost (binary classification)
xgb_param_dict = {
    "n_estimators": [100, 200],
    "learning_rate": [0.05, 0.1],
    "max_depth": [3, 5],
}
models_and_grids.append(("XGBoost", XGBClassifier, xgb_param_dict))

# 8) KNN
knn_param_dict = {
    "n_neighbors": [3, 5],
    "weights": ["uniform", "distance"],
    "p": [1, 2],  # 1 = Manhattan, 2 = Euclidean
}
models_and_grids.append(("KNN", KNeighborsClassifier, knn_param_dict))

# 9) Voting Ensemble (LR + RF + SVM)
#    Base estimators are fixed; we tune voting + weights.
ensemble_param_dict = {
    "voting": ["hard", "soft"],
    "weights": [(1, 1, 1), (2, 1, 1)],
    "flatten_transform": [True, False],
}
# We'll build the VotingClassifier manually inside the loop.

# 7. Run experiments and log to MLflow

for family, ModelClass, param_dict in models_and_grids:
    param_grid = make_param_grid(param_dict)
    print(f"\n==============================")
    print(f"Running {family} with {len(param_grid)} hyperparameter combos")
    print(f"==============================")

    for params in param_grid:
        run_name = f"{family}_{params}"

        with mlflow.start_run(run_name=run_name):
            # Some models need specific extra args
            if family == "LogisticRegression":
                model = ModelClass(
                    max_iter=1000,
                    solver="lbfgs",
                    C=params["C"],
                    class_weight=params["class_weight"],
                    penalty=params["penalty"],
                )
            elif family == "SVM":
                model = ModelClass(
                    C=params["C"],
                    kernel=params["kernel"],
                    gamma=params["gamma"],
                    probability=True,  # needed for ensemble soft voting later
                )
            elif family == "ANN":
                model = ModelClass(
                    hidden_layer_sizes=params["hidden_layer_sizes"],
                    alpha=params["alpha"],
                    activation=params["activation"],
                    max_iter=1000,
                    random_state=42,
                )
            elif family == "RandomForest":
                model = ModelClass(
                    n_estimators=params["n_estimators"],
                    max_depth=params["max_depth"],
                    max_features=params["max_features"],
                    random_state=42,
                )
            elif family == "XGBoost":
                model = ModelClass(
                    n_estimators=params["n_estimators"],
                    learning_rate=params["learning_rate"],
                    max_depth=params["max_depth"],
                    use_label_encoder=False,
                    eval_metric="logloss",
                    random_state=42,
                )
            elif family == "KNN":
                model = ModelClass(
                    n_neighbors=params["n_neighbors"],
                    weights=params["weights"],
                    p=params["p"],
                )
            elif family == "NaiveBayes":
                model = ModelClass(
                    var_smoothing=params["var_smoothing"]
                )
            elif family == "DecisionTree":
                model = ModelClass(
                    max_depth=params["max_depth"],
                    min_samples_split=params["min_samples_split"],
                    criterion=params["criterion"],
                    random_state=42,
                )
            else:
                model = ModelClass(**params)

            # Fit + predict
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)

            metrics = evaluate_model(run_name, y_test, y_pred)

            # Log params & metrics
            mlflow.log_param("model_family", family)
            for k, v in params.items():
                mlflow.log_param(k, v)
            mlflow.log_metrics(metrics)

            # Save model artifact
            mlflow.sklearn.log_model(model, artifact_path="model")

            # Update best model by recall
            update_best(family, run_name, params, metrics)

# 8. Run ensemble experiments separately (needs fitted base estimators)

print("\n==============================")
print("Running Voting Ensemble models")
print("==============================")

# Refit some strong base learners with default-ish params
base_lr = LogisticRegression(max_iter=1000, C=1.0, class_weight="balanced", penalty="l2")
base_rf = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42)
base_svm = SVC(C=1.0, kernel="rbf", gamma="scale", probability=True, random_state=42)

base_lr.fit(X_train, y_train)
base_rf.fit(X_train, y_train)
base_svm.fit(X_train, y_train)

ensemble_param_grid = make_param_grid(ensemble_param_dict)

for params in ensemble_param_grid:
    run_name = f"Ensemble_{params}"

    with mlflow.start_run(run_name=run_name):
        ensemble = VotingClassifier(
            estimators=[
                ("lr", base_lr),
                ("rf", base_rf),
                ("svm", base_svm),
            ],
            voting=params["voting"],
            weights=params["weights"],
            flatten_transform=params["flatten_transform"],
        )

        ensemble.fit(X_train, y_train)
        y_pred = ensemble.predict(X_test)

        metrics = evaluate_model(run_name, y_test, y_pred)

        mlflow.log_param("model_family", "VotingEnsemble")
        for k, v in params.items():
            mlflow.log_param(k, v)
        mlflow.log_metrics(metrics)
        mlflow.sklearn.log_model(ensemble, artifact_path="model")

        update_best("VotingEnsemble", run_name, params, metrics)

# 9. Summary: best model by Recall
print("\nüèÜ BEST MODEL (by Recall):")
print("----------------------------")
print("Model Family :", best_overall["family"])
print("Run Name     :", best_overall["run_name"])
print("Params       :", best_overall["params"])
print("Metrics      :", best_overall["metrics"])

print("""
üìå Note:
For this healthcare screening problem, we prioritize **Recall (Sensitivity)** ‚Äî
catching as many true positive cervical cancer cases as possible ‚Äî even if that
means a small trade-off in precision.
""")